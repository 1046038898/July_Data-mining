{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奇异值矩阵的秩不一定等于自己设定的主题个数\n",
    "\n",
    "第一个用户在第一个主题(科幻)上的得分，第二个主题(言情)上的得分，第三个主题上的得分(这个主题我们不知道)\n",
    "\n",
    "U矩阵的每一列都是单位正交基"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 为什么要数据降维\n",
    "* 为什么能数据降维\n",
    "* SVD\n",
    "  * 基本概念与性质\n",
    "  * 怎么用SVD降维\n",
    "  * 实际案例\n",
    "* CUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么要降维\n",
    "\n",
    "* 海量数据太大，不得不降维。\n",
    "* 可以让你使用简单的模型运算的更快，更容易理解，更容易维护。\n",
    "* 优质的降维数据可以在使用不是最优的模型参数的情况下得到不错的预测结果，这样你就不必费力去选择最适合的模型和最优的参数了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么能降维\n",
    "<img src='727_1.png'>\n",
    "**假设：数据实际上是存在或者靠近一个低维子空间中,那么子空间的坐标轴能够最有效地表达这个数据。**\n",
    "\n",
    "如左图所示，二维空间中，数据基本集中在一条直线附近，这时候二维空间就可以降到一维。(可能会有部分损失)\n",
    "\n",
    "如右图所示，三维空间可以降到二维空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回忆：矩阵的秩\n",
    "\n",
    "**矩阵的秩：矩阵的线性独立的列（行）的个数。**\n",
    "\n",
    "$$\n",
    "\\mathbf{A}=\\left[\\begin{array}{ccc}{1} & {2} & {1} \\\\ {-2} & {-3} & {1} \\\\ {3} & {5} & {0}\\end{array}\\right]\n",
    "$$\n",
    "上述矩阵的秩r=2\n",
    "\n",
    "* 为什么关心矩阵的秩？\n",
    "\n",
    "  * 我们可以把A用两个新的基向量表示：\n",
    "  * [1 2 1] [-2 -3 1]\n",
    " 因为矩阵的第三行等于第一行减去第二行的数值，我们用前两行表示基向量。\n",
    "  * 则相应的坐标就变为: [1 0] [0 1] [1 1]\n",
    " \n",
    "[1 0]：只取第一行的坐标，不要第二行的坐标\n",
    "\n",
    "[0 1]：只取第二行的坐标，不要第一行的坐标\n",
    "\n",
    "[1 1]：第一行和第二行的坐标相加\n",
    "\n",
    "**用两个坐标表达原来三个坐标表示的，客观上降维**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 秩其实就是最小的维度\n",
    "\n",
    "* 观察三维视图：\n",
    "  * 把矩阵的每一行作为一点在三维空间的坐标\n",
    "<img src='727_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我们可以重新采用一个坐标系，这个坐标系是以原矩阵的前两行作为坐标基底的。\n",
    "  * 老坐标基底：[1 0 0] [0 1 0] [0 0 1]\n",
    "  * 新坐标基底：[1 2 1] [-2 -3 1]\n",
    "  \n",
    "* 则ABC三点的新坐标分别为:\n",
    "  * A: [1 0],\n",
    "  * B: [0 1],\n",
    "  * C: [1 1]\n",
    "  \n",
    "**注意：我们已经把ABC三点降维了**\n",
    "<img src='727_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维的关键\n",
    "\n",
    "**降维的关键就是找到能够表达数据的最少维度，用最少的坐标轴表示数据。**\n",
    "\n",
    "下图的点在二维空间中，但大量聚集在红线附近，所以就可以用红线所代表的一维坐标来表示。\n",
    "\n",
    "当然这样做有一点误差\n",
    "<img src='727_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n",
    "<img src='727_5.png'>\n",
    "\n",
    "**A：输入矩阵**\n",
    "\n",
    "$m \\times n$(e.g.,m篇文章，n个词语)\n",
    "\n",
    "**U：左奇异向量矩阵**\n",
    "\n",
    "$m \\times r$(m篇文章,r个主题)\n",
    "\n",
    "**$\\Sigma$：奇异值矩阵**\n",
    "\n",
    "$r \\times r$对角阵(每个主题的重要性)(r:矩阵A的秩)\n",
    "\n",
    "**V：右奇异向量矩阵**\n",
    "\n",
    "$n \\times r$(n个词语，r个主题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 奇异值分解\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\approx \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{T}=\\sum_{i} \\sigma_{i} \\mathbf{u}_{i} \\circ \\mathbf{v}_{i}^{\\mathrm{T}}\n",
    "$$\n",
    "<img src='727_6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD的性质\n",
    "\n",
    "对于一个实数矩阵，总能够拆解成三个矩阵相乘$\\mathrm{A}=\\mathrm{U} \\sum \\mathrm{V}_{\\mathrm{T}}$\n",
    "\n",
    "这三个矩阵满足如下性质：\n",
    "\n",
    "**$\\mathrm{U} . \\Sigma \\cdot \\mathrm{V}$是唯一的**\n",
    "\n",
    "**$\\mathrm{U} . \\mathrm{V}$的列是单位标准正交基**\n",
    "\n",
    "**$\\sum$是对角阵，对角上的每一个值是奇异值，是正数，并且按降序排列。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD案例：用户看电影\n",
    "<img src='727_7.png'>\n",
    "\n",
    "从图中可以看出，A矩阵每行代表一个用户，每列分别代表电影黑客帝国、异形、冲出宁静号、卡萨布兰卡、天使爱美丽。也就是说整个**矩阵代表每位用户是否浏览过对应的电影，没有浏览过则记为0，浏览过则计数。**\n",
    "\n",
    "这里我们先把电影分成两个主题：科幻和言情。主题也可以称为隐藏维度或者隐藏因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='727_8.png'>\n",
    "\n",
    "其中U矩阵是m*r,m代表用户，r代表主体，这里我们划分的主题是两个，为什么这里r是3呢？这是因为实际进行SVD得出的主题个数。第一列代表科幻主题，第二列代表言情主题，第三列是我们不知道的主题。哪位用户在哪个主题上给予的权重越多，表明他越喜欢这类主题。所以**U是\"用户-主题\"相似矩阵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**其中，奇异值矩阵表示主题的强度：第一列表示科幻主题的强度，第二列表示言情主题的强度，第三列表示未知主题的强度。**\n",
    "<img src='727_9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，右奇异矩阵V表示\"电影-主题\"相似矩阵\n",
    "<img src='727_10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD的深入理解\n",
    "\n",
    "* “电影”、“用户”和“主题”\n",
    "  * U:“用户-主题”相似矩阵\n",
    "  * V:“电影-主题”相似矩阵\n",
    "  * Σ：其对角元素是每一个主题的“强度”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD进行降维\n",
    "\n",
    "SVD能够给出“最好”的结果,所谓“最好”就是使得平方误差（投影）最小\n",
    "\n",
    "* SVD怎么降维？能够精确到什么程度？\n",
    "* A:**把最小的奇异值设为0**\n",
    "\n",
    "把最小的奇异值设为0，相应的r就要减少一列。\n",
    "\n",
    "**这样减少了一列，原本的等式就变成了约等式**\n",
    "<img src='728_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终我们将两边的结果进行比较：\n",
    "\n",
    "<img src='728_2.png'>\n",
    "\n",
    "那么我们如何评价得到的这个结果？这就用到了弗罗宾尼斯范数\n",
    "$$\n",
    "| \\mathrm{Ml}_{\\mathrm{F}}=\\sqrt{\\Sigma_{\\mathrm{ij}} \\mathrm{M}_{\\mathrm{ij}}^{2}}\n",
    "$$\n",
    "$$\n",
    "\\| A-B l_{F}=\\sqrt{\\Sigma_{i j}\\left(A_{i j}-B_{i j}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "越小越好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD:最好的低秩近似\n",
    "\n",
    "<img src='728_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保留多少奇异值\n",
    "\n",
    "80-90%的能量=$\\sum_{i} \\sigma_{i}^{2}$\n",
    "\n",
    "* 一般经验值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD:计算复杂度\n",
    "\n",
    "* 完全计算复杂度：\n",
    "  * O(nm2)或O(n2m)，取最小的。\n",
    "* 但是我们可以减少计算量：\n",
    "  * 如果只需要奇异值\n",
    "  * 或者只需要前k个奇异向量\n",
    "  * 或者矩阵是稀疏矩阵\n",
    "* 一般用开源线性运算算法包：\n",
    "  * LINPACK, SciPy,Matlab, SPlus, Mathematica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例：给用户推荐电影\n",
    "\n",
    "* Q:找到潜在喜欢“黑客帝国”电影的用户\n",
    "* A:将用户的搜索统计映射到“主题空间”\n",
    "\n",
    "<img src='728_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说，我们有\n",
    "<img src='728_5.png'>\n",
    "<img src='728_6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='728_7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察：用户d与用户q是相似的，虽然他们在原始坐标下是正交的！\n",
    "<img src='728_8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD降维的特点\n",
    "\n",
    "* 奇异向量\n",
    "  * 每一个奇异向量是所有输入矩阵的行向量或列向量的线性组合\n",
    "* 奇异向量是稠密的\n",
    "<img src='728_9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD和PCA之间的区别和相同点？\n",
    "\n",
    "**SVD进行分解的右奇异矩阵V就是PCA协方差矩阵的特征向量矩阵**\n",
    "\n",
    "PCA能用的地方SVD一定能用，SVD能用的地方PCA不一定能用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何计算PCA？\n",
    "\n",
    "https://blog.csdn.net/program_developer/article/details/80632779\n",
    "\n",
    "先来看几个公式：\n",
    "\n",
    "样本均值：\n",
    "$$\n",
    "\\overline{x}=\\frac{1}{n} \\sum_{i=1}^{N} x_{i}\n",
    "$$\n",
    "样本方差：\n",
    "$$\n",
    "S^{2}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}\n",
    "$$\n",
    "样本X和Y的协方差矩阵\n",
    "$$\n",
    "\\begin{aligned} \\operatorname{Cov}(X, Y) &=E[(X-E(X))(Y-E(Y))] \\\\ &=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)\\left(y_{i}-\\overline{y}\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "**协方差为正时，说明X和Y是正相关关系；协方差为负时，说明X和Y是负相关关系；协方差为0时，说明X和Y是相互独立。Cov(X,X)就是X的方差。当样本是n维数据时，它们的协方差实际上是协方差矩阵(对称方阵)。例如，对于3维数据(x,y,z)，计算它的协方差就是：**\n",
    "$$\n",
    "\\operatorname{Cov}(X, Y, Z)=\\left[\\begin{array}{lll}{\\operatorname{Cov}(x, x)} & {\\operatorname{Cov}(x, y)} & {\\operatorname{Cov}(x, z)} \\\\ {\\operatorname{Cov}(y, x)} & {\\operatorname{Cov}(y, y)} & {\\operatorname{Cov}(y, z)} \\\\ {\\operatorname{Cov}(z, x)} & {\\operatorname{Cov}(z, y)} & {\\operatorname{Cov}(z, z)}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "### (1) 基于特征值分解协方差矩阵实现PCA算法\n",
    "输入：数据集$X=\\left\\{x_{1}, x_{2}, x_{3}, \\dots, x_{n}\\right\\}$，需要降到K维。\n",
    "* 1.去平均值(即去中心化)，即每一维特征减去各自的平均值。\n",
    "* 2.计算协方差矩阵$\\frac{1}{n} X X^{T}$\n",
    "* 3.用特征值分解方法求协方差矩阵$\\frac{1}{n} X X^{T}$的特征值与特征向量\n",
    "* 4.对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P。\n",
    "* 5.将数据转换到k个特征向量构建的新空间中，即Y=PX。\n",
    "\n",
    "#### 举个例子：\n",
    "$$\n",
    "X=\\left(\\begin{array}{ccccc}{-1} & {-1} & {0} & {2} & {0} \\\\ {-2} & {0} & {0} & {1} & {1}\\end{array}\\right)\n",
    "$$\n",
    "以X为例，我们用PCA方法将这两行数据降到一行。\n",
    "\n",
    "1)因为X矩阵的每行已经是零均值，所以不需要去平均值。\n",
    "\n",
    "2)求协方差矩阵：\n",
    "$$\n",
    "C=\\frac{1}{5}\\left(\\begin{array}{ccccc}{-1} & {-1} & {0} & {2} & {0} \\\\ {-2} & {0} & {0} & {1} & {1}\\end{array}\\right)\\left(\\begin{array}{cc}{-1} & {-2} \\\\ {-1} & {0} \\\\ {0} & {0} \\\\ {2} & {1} \\\\ {0} & {1}\\end{array}\\right)=\\left(\\begin{array}{cc}{\\frac{6}{5}} & {\\frac{4}{5}} \\\\ {\\frac{4}{5}} & {\\frac{6}{5}}\\end{array}\\right)\n",
    "$$\n",
    "3)求协方差矩阵的特征值与特征向量。\n",
    "\n",
    "求解后的特征值为：\n",
    "$$\n",
    "\\lambda_{1}=2, \\quad \\lambda_{2}=\\frac{2}{5}\n",
    "$$\n",
    "对应的特征向量为：\n",
    "$$\n",
    "c_{1}\\left(\\begin{array}{l}{1} \\\\ {1}\\end{array}\\right), c_{2}\\left(\\begin{array}{c}{-1} \\\\ {1}\\end{array}\\right)\n",
    "$$\n",
    "其中对应的特征向量分别是一个通解，$\\boldsymbol{c}_{1}$和$\\boldsymbol{c}_{2}$可以取任意实数。那么标准化后的特征向量为：\n",
    "$$\n",
    "\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{2}}} \\\\ {\\frac{1}{\\sqrt{2}}}\\end{array}\\right),\\left(\\begin{array}{c}{-\\frac{1}{\\sqrt{2}}} \\\\ {\\frac{1}{\\sqrt{2}}}\\end{array}\\right)\n",
    "$$\n",
    "4)矩阵P为：\n",
    "$$\n",
    "P=\\left(\\begin{array}{cc}{\\frac{1}{\\sqrt{2}}} & {\\frac{1}{\\sqrt{2}}} \\\\ {-\\frac{1}{\\sqrt{2}}} & {\\frac{1}{\\sqrt{2}}}\\end{array}\\right)\n",
    "$$\n",
    "5)最后我们用P的第一行乘以数据矩阵X，就得到了降维后的表示：\n",
    "$$\n",
    "Y=\\left(\\begin{array}{cc}{\\frac{1}{\\sqrt{2}}} & {\\frac{1}{\\sqrt{2}}}\\end{array}\\right)\\left(\\begin{array}{ccccc}{-1} & {-1} & {0} & {2} & {0} \\\\ {-2} & {0} & {0} & {1} & {1}\\end{array}\\right)=\\left(\\begin{array}{ccc}{-\\frac{3}{\\sqrt{2}}} & {-\\frac{1}{\\sqrt{2}}} & {0} & {\\frac{3}{\\sqrt{2}}} & {-\\frac{1}{\\sqrt{2}}}\\end{array}\\right)\n",
    "$$\n",
    "用图形表示如下：\n",
    "<img src='728_10.png'>\n",
    "#### 实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "协方差矩阵\n",
      " [[ 0.66666667 -0.33333333]\n",
      " [-0.33333333  0.66666667]]\n",
      "特征值\n",
      " [1.         0.33333333]\n",
      "特征向量\n",
      " [[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "------------------------------\n",
      "原矩阵\n",
      " [[-7.07106781e-01]\n",
      " [ 5.55111512e-17]\n",
      " [ 7.07106781e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def pca(X,k):\n",
    "    #k is the components you want\n",
    "    n_samples, n_features = X.shape\n",
    "    mean=np.array([np.mean(X[:,i]) for i in range(n_features)]) #计算每列的均值\n",
    "    norm_X=X-mean #去中心化\n",
    "    scatter_matrix=np.dot(np.transpose(norm_X),norm_X) #计算协方差矩阵\n",
    "    eig_val, eig_vec = np.linalg.eig(scatter_matrix) #计算特征值和特征向量\n",
    "    print('协方差矩阵\\n',scatter_matrix)\n",
    "    print('特征值\\n',eig_val)\n",
    "    print('特征向量\\n',eig_vec)\n",
    "    print('------------------------------')\n",
    "    eig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(n_features)]#把每组特征值和特征向量分开来\n",
    "    eig_pairs.sort(reverse=True) #将特征值从大到小排序\n",
    "    feature=np.array([ele[1] for ele in eig_pairs[:k]]) #选择其中最大的K个特征值所对应的特征向量\n",
    "    data=np.dot(norm_X,np.transpose(feature))# 降维之后的矩阵\n",
    "    return data\n",
    "X = np.array([[0,1], [1,1], [1,0]])\n",
    "print('原矩阵\\n',pca(X,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn实现的PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50917706]\n",
      " [ 2.40151069]\n",
      " [ 3.7751606 ]\n",
      " [-1.20075534]\n",
      " [-2.05572155]\n",
      " [-3.42937146]]\n"
     ]
    }
   ],
   "source": [
    "##用sklearn的PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "X = np.array([[-1, 1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca=PCA(n_components=1).fit(X)\n",
    "print(pca.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 基于SVD分解协方差矩阵实现PCA算法\n",
    "\n",
    "上面我们说过**SVD进行分解的右奇异矩阵V就是PCA协方差矩阵的特征向量矩阵**，我们计算出V右奇异矩阵，选择K个组成P向量，就可以计算出降维之后的矩阵\n",
    "\n",
    "#### SCD的python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,1], [1,1],[1,0]])\n",
    "u,s,v = np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.08248290e-01,  7.07106781e-01,  5.77350269e-01],\n",
       "       [-8.16496581e-01,  2.64811510e-17, -5.77350269e-01],\n",
       "       [-4.08248290e-01, -7.07106781e-01,  5.77350269e-01]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.73205081, 1.        ])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678, -0.70710678],\n",
       "       [-0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD计算举例\n",
    "\n",
    "https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg\n",
    "\n",
    "这里我们用一个简单的矩阵来说明奇异值分解的步骤。我们的矩阵A定义为：\n",
    "$$\n",
    "A=\\left(\\begin{array}{ll}{0} & {1} \\\\ {1} & {1} \\\\ {1} & {0}\\end{array}\\right)\n",
    "$$\n",
    "首先，我们求出$A^{T} A$和$A A^{T}$:\n",
    "$$\n",
    "A^{T} A=\\left(\\begin{array}{lll}{0} & {1} & {1} \\\\ {1} & {1} & {0}\\end{array}\\right)\\left(\\begin{array}{ll}{0} & {1} \\\\ {1} & {1} \\\\ {1} & {0}\\end{array}\\right)=\\left(\\begin{array}{ll}{2} & {1} \\\\ {1} & {2}\\end{array}\\right)\n",
    "$$\n",
    "$$\n",
    "A A^{T}=\\left(\\begin{array}{cc}{0} & {1} \\\\ {1} & {1} \\\\ {1} & {0}\\end{array}\\right)\\left(\\begin{array}{ccc}{0} & {1} & {1} \\\\ {1} & {1} & {0}\\end{array}\\right)=\\left(\\begin{array}{ccc}{1} & {1} & {0} \\\\ {1} & {2} & {1} \\\\ {0} & {1} & {1}\\end{array}\\right)\n",
    "$$\n",
    "然后，求出$A^{T} A$和$A A^{T}的特征值和特征向量：\n",
    "\n",
    "$A^{T} A$的特征值和特征向量：\n",
    "\n",
    "$$\n",
    "\\lambda_{1}=3 ; \\quad v_{1}=\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{2}}} \\\\ {\\frac{1}{\\sqrt{2}}}\\end{array}\\right) ; \\lambda_{2}=1 ; \\quad v_{2}=\\left(\\begin{array}{c}{\\frac{-1}{\\sqrt{2}}} \\\\ {\\frac{1}{\\sqrt{2}}}\\end{array}\\right)\n",
    "$$\n",
    "$A A^{T}的特征值和特征向量：\n",
    "\n",
    "$$\n",
    "\\lambda_{1}=3 ; u_{1}=\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{6}}} \\\\ {\\frac{2}{\\sqrt{6}}} \\\\ {\\frac{1}{\\sqrt{6}}}\\end{array}\\right) ; \\lambda_{2}=1 ; u_{2}=\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{2}}} \\\\ {0} \\\\ {\\frac{-1}{\\sqrt{2}}}\\end{array}\\right) ; \\lambda_{3}=0 ; u_{3}=\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{3}}} \\\\ {\\frac{-1}{\\sqrt{3}}} \\\\ {\\frac{1}{\\sqrt{3}}}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "其次，我们利用$A v_{i}=\\sigma_{i} u_{i}, \\quad i=1,2$，求奇异值：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{\\left(\\begin{array}{cc}{0} & {1} \\\\ {1} & {1} \\\\ {1} & {0}\\end{array}\\right)\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{2}}} \\\\ {\\frac{1}{\\sqrt{2}}}\\end{array}\\right)=\\sigma_{1}\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{6}}} \\\\ {\\frac{2}{\\sqrt{6}}} \\\\ {\\frac{1}{\\sqrt{6}}}\\end{array}\\right) \\Rightarrow \\sigma_{1}=\\sqrt{3}} \\\\ {\\left(\\begin{array}{cc}{0} & {1} \\\\ {1} & {1} \\\\ {1} & {0}\\end{array}\\right)\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{2}}} \\\\ {\\frac{1}{\\sqrt{2}}}\\end{array}\\right)=\\sigma_{2}\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{2}}} \\\\ {\\frac{1}{\\sqrt{2}}} \\\\ {\\frac{1}{\\sqrt{2}}} \\\\ {0} \\\\ {\\frac{-1}{\\sqrt{2}}}\\end{array}\\right) \\Rightarrow \\sigma_{2}=1}\\end{array}\n",
    "$$\n",
    "\n",
    "最后，我们得到A的奇异值分解为：\n",
    "\n",
    "$$\n",
    "A=U \\Sigma V^{T}=\\left(\\begin{array}{ccc}{\\frac{1}{\\sqrt{6}}} & {\\frac{1}{\\sqrt{2}}} & {\\frac{1}{\\sqrt{3}}} \\\\ {\\frac{2}{\\sqrt{6}}} & {0} & {\\frac{-1}{\\sqrt{3}}} \\\\ {\\frac{1}{\\sqrt{6}}} & {\\frac{-1}{\\sqrt{2}}} & {\\frac{1}{\\sqrt{3}}}\\end{array}\\right)\\left(\\begin{array}{cc}{\\sqrt{3}} & {0} \\\\ {0} & {1} \\\\ {0} & {0}\\end{array}\\right)\\left(\\begin{array}{cc}{\\frac{1}{\\sqrt{2}}} & {\\frac{1}{\\sqrt{2}}} \\\\ {\\frac{-1}{\\sqrt{2}}} & {\\frac{1}{\\sqrt{2}}}\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
